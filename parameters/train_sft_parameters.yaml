original_model_path: "models/gpt2-xl"
new_model_path: "models/gpt2-xl-D"

dataset_path: "data/processed/spells_processed_and_formatted_train.jsonl"

################################################################################
# General parameters
################################################################################

load_in_8_bit: false
max_seq_length: 512
packing: true

################################################################################
# Pretrained model parameters
################################################################################

pretrained_parameters:
  torch_dtype: "f32"

################################################################################
# Instruction format parameters
################################################################################

input_format: "alpaca"
input_format_parameters:
  instruction_field: "instruction"
  response_field: "response"

################################################################################
# TrainingArguments parameters
################################################################################

training_arguments:

  num_train_epochs: 3

  per_device_train_batch_size: 32

  gradient_accumulation_steps: 4

  gradient_checkpointing: true

  optim: "adamw_torch"

  bf16: true
  fp16: false
  tf32: true

  learning_rate: 0.00005

  lr_scheduler_type: "cosine"

  warmup_ratio: 0.0

  weight_decay: 0.0001

  max_grad_norm: 1.0

  logging_steps: 5

  seed: 42